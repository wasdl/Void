# https://colab.research.google.com/drive/1UTG0jsTU6SMeLqPHQ0GZybEAuCiNk2LI?usp=sharing


import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

import torch
from torch import nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error
from matplotlib import pyplot as plt
import pickle
from app.config import get_paths_dict

class EncoderModel(nn.Module):
    """ì˜¤í† ì¸ì½”ë” PyTorch ëª¨ë¸"""
    def __init__(self, input_dim=48, latent_dim=12):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 24),
            nn.ReLU(),
            nn.Linear(24, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 24),
            nn.ReLU(),
            nn.Linear(24, input_dim)
        )

    def forward(self, x):
        return self.decoder(self.encoder(x))

class AutoencoderClassifier:
    """íŒ¨í„´ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ë¶„ë¥˜ê¸°"""
    def __init__(self):
        self.models = {}
        self.thresholds = {}
        self.patterns = ['A', 'B', 'C', 'D']
        self.paths = get_paths_dict()
        self.model_path = os.path.join(self.paths.get('models', ''), 'autoencoder')
        os.makedirs(self.model_path, exist_ok=True)
        
        # ì„ê³„ê°’ ì¡°ì • ê´€ë ¨ ì„¤ì • (DQM ë¬¸ì„œ ê¸°ì¤€ ì ìš©)
        self.std_multipliers = [1.5, 1.8, 2.0, 2.3, 2.5]
        self.default_std_multiplier = 1.5  # DQM ë¬¸ì„œ ê¸°ì¤€: Mean + 1.5*Std
        self.unknown_ratio_max = 0.05  # DQM ë¬¸ì„œ ê¸°ì¤€: 5%
        self.max_retries = 3
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¶”ì 
        self.pipeline_status = {
            'current_stage': 'init',
            'last_error': None,
            'threshold_history': {},
            'needs_dbscan_adjustment': False
        }
    
    def train_autoencoder(self, model, data, epochs=150, lr=1e-3):
        """ë‹¨ì¼ ì˜¤í† ì¸ì½”ë” ëª¨ë¸ í›ˆë ¨"""
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)
        model.train()
        inputs = torch.tensor(data, dtype=torch.float32)

        for _ in range(epochs):
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, inputs)
            loss.backward()
            optimizer.step()
    
    def _load_pattern_data(self, patterns=None):
        """ì›ë³¸ íŒ¨í„´ ë°ì´í„° ë¡œë“œ (íŠ¹ì„± ë°ì´í„°ê°€ ì•„ë‹Œ ì›ë³¸ ì‹œê³„ì—´)"""
        data_path = self.paths.get('data', '')
        
        # íŒ¨í„´ë³„ íŒŒì¼ ì°¾ê¸°
        pattern_files = []
        if patterns:
            # ì§€ì •ëœ íŒ¨í„´ë§Œ ì°¾ê¸°
            for pattern in patterns:
                file_path = os.path.join(data_path, f"pattern_{pattern}_30days.csv")
                if os.path.exists(file_path):
                    pattern_files.append(file_path)
        else:
            # ëª¨ë“  íŒ¨í„´ íŒŒì¼ ì°¾ê¸°
            all_files = [f for f in os.listdir(data_path) 
                       if f.startswith('pattern_') and f.endswith('_30days.csv')]
            pattern_files = [os.path.join(data_path, f) for f in all_files]
        
        if not pattern_files:
            return None
            
        # ëª¨ë“  íŒŒì¼ ë¡œë“œ ë° ë³‘í•©
        all_dfs = []
        for file_path in pattern_files:
            try:
                df = pd.read_csv(file_path)
                all_dfs.append(df)
            except Exception as e:
                print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {file_path}: {str(e)}")
                
        if not all_dfs:
            return None
            
        return pd.concat(all_dfs, ignore_index=True)
    
    def train_classifiers(self, patterns=None):
        """ê° íŒ¨í„´ë³„ ì˜¤í† ì¸ì½”ë” í•™ìŠµ"""
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
        self.pipeline_status = {
            'current_stage': 'training_started',
            'last_error': None,
            'threshold_history': {},
            'needs_dbscan_adjustment': False
        }
        
        # íŒ¨í„´ ë°ì´í„° ë¡œë“œ
        raw_data = self._load_pattern_data(patterns)
        
        if raw_data is None:
            self.pipeline_status['current_stage'] = 'data_loading_failed'
            self.pipeline_status['last_error'] = "íŒ¨í„´ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return {"status": "error", "error": "íŒ¨í„´ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        # íŒ¨í„´ë³„ ë°ì´í„° í™•ì¸
        patterns_in_data = set(raw_data['pattern'].unique())
        patterns_to_train = [p for p in self.patterns if p in patterns_in_data]
        
        if not patterns_to_train:
            self.pipeline_status['current_stage'] = 'no_valid_patterns'
            self.pipeline_status['last_error'] = "í›ˆë ¨í•  íŒ¨í„´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            return {"status": "error", "error": "í›ˆë ¨í•  íŒ¨í„´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ë°ì´í„° ì „ì²˜ë¦¬ - ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„° ì‚¬ìš©
        X_5day, y_5day = self._prepare_data(raw_data, patterns_to_train)
        
        if len(X_5day) == 0:
            self.pipeline_status['current_stage'] = 'insufficient_data'
            self.pipeline_status['last_error'] = "ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            return {"status": "error", "error": "ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
        
        # íŒ¨í„´ë³„ ëª¨ë¸ í›ˆë ¨
        for pattern in patterns_to_train:
            print(f"íŒ¨í„´ {pattern} ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...")
            data = X_5day[y_5day == pattern]
            
            if len(data) == 0:
                print(f"íŒ¨í„´ {pattern}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
                
            model = EncoderModel()
            self.train_autoencoder(model, data)
            self.models[pattern] = model

            # ì„ê³„ê°’ ì„¤ì • (ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘)
            model.eval()
            with torch.no_grad():
                outputs = model(torch.tensor(data, dtype=torch.float32)).numpy()
            mses = [mean_squared_error(x, y) for x, y in zip(data, outputs)]
            mse_mean = np.mean(mses)
            mse_std = np.std(mses)
            
            # ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì • - DQM ë¬¸ì„œ ê¸°ì¤€ (Mean + 1.5*Std)
            self.thresholds[pattern] = mse_mean + self.default_std_multiplier * mse_std
        
        self.pipeline_status['current_stage'] = 'training_completed'
        
        # ì„ê³„ê°’ ìµœì í™” ë° Unknown ë¹„ìœ¨ ê²€ì¦
        threshold_result = self._optimize_thresholds(X_5day, y_5day)
        
        if threshold_result["status"] == "error":
            print(f"ê²½ê³ : {threshold_result.get('message', 'ì„ê³„ê°’ ìµœì í™” ì‹¤íŒ¨')}")
            print("ê¸°ë³¸ ì„ê³„ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # ë‹¤ìŒ ë‹¨ê³„ì—ì„œ DBSCAN ì¬ì¡°ì • í•„ìš” ì—¬ë¶€ ì„¤ì •
            if threshold_result.get("recommendation") == "DBSCAN íŒŒë¼ë¯¸í„° ì¬ì¡°ì • í•„ìš”":
                self.pipeline_status['needs_dbscan_adjustment'] = True
        
        # ëª¨ë¸ í‰ê°€
        if len(self.models) > 0:
            self._evaluate_models(X_5day, y_5day, patterns_to_train)
            
        # ëª¨ë¸ ì €ì¥
        self._save_models()
        
        # íŒŒì´í”„ë¼ì¸ ì—°ê³„ë¥¼ ìœ„í•œ ìƒíƒœ ì¶”ê°€
        return {
            "status": "success" if threshold_result["status"] == "success" else "warning",
            "message": f"{len(self.models)}ê°œ íŒ¨í„´ì— ëŒ€í•œ ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì™„ë£Œ",
            "patterns_trained": list(self.models.keys()),
            "threshold_result": threshold_result,
            "pipeline_status": self.pipeline_status,
            "needs_dbscan_adjustment": self.pipeline_status['needs_dbscan_adjustment']
        }
    
    def _prepare_data(self, df, patterns, size=1000, day_range=30):
        """ë°ì´í„° ì „ì²˜ë¦¬ - ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„° ì‚¬ìš©"""
        X_5day = []
        y_5day = []
        
        # ì‹œì‘ ë‚ ì§œ ì„¤ì •
        try:
            df['date'] = pd.to_datetime(df['date'])
            start_date = df['date'].min()
        except:
            # configì—ì„œ ê¸°ë³¸ ë‚ ì§œ ê°€ì ¸ì˜¤ê±°ë‚˜ í˜„ì¬ ë‚ ì§œ ì‚¬ìš©
            start_date = datetime.now() - timedelta(days=30)
        
        # íŒ¨í„´ë³„ ì²˜ë¦¬
        for pattern in patterns:
            pattern_df = df[df['pattern'] == pattern]
            
            if len(pattern_df) == 0:
                continue
                
            # ì‚¬ìš©ì ID ìƒ˜í”Œë§
            person_ids = pattern_df['person_id'].unique()
            if len(person_ids) > size:
                person_ids = np.random.choice(person_ids, size, replace=False)
            
            # ê° ì‚¬ìš©ìë³„ ë°ì´í„° ì²˜ë¦¬
            for person_id in person_ids:
                person_df = pattern_df[pattern_df['person_id'] == person_id].sort_values(['date', 'hour', 'minute'])
                
                for i in range(0, day_range, 5):  # 0~4, 5~9, ..., 25~29
                    day_range_slice = (i, i + 5)
                    temp_df = person_df[
                        ((person_df['date'] - start_date).dt.days >= day_range_slice[0]) &
                        ((person_df['date'] - start_date).dt.days < day_range_slice[1])
                    ]

                    if len(temp_df) == 48 * 5:  # 5ì¼ì¹˜ ì „ì²´ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                        # ì¶œìˆ˜ëŸ‰ ì‹œê³„ì—´ë¡œ ë³€í™˜ (ì‹œê°„ë³„ ì •ë ¬)
                        amount_series = temp_df['amount'].values
                        
                        # ì •ê·œí™”
                        max_amount = max(amount_series) if max(amount_series) > 0 else 1
                        norm_series = amount_series / max_amount
                        
                        # í•˜ë£¨ ë‹¨ìœ„ë¡œ ì••ì¶• (48 ìŠ¬ë¡¯)
                        daily_pattern = []
                        for day in range(5):
                            day_start = day * 48
                            day_end = day_start + 48
                            day_data = norm_series[day_start:day_end]
                            
                            # ëª¨ë“  ë‚ ì˜ íŒ¨í„´ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ íŒ¨í„´ìœ¼ë¡œ ë§Œë“¦
                            if day == 0:
                                daily_pattern = day_data
                            else:
                                daily_pattern = daily_pattern + day_data
                                
                        # ì¼ë³„ í•©ì‚°í•œ íŒ¨í„´ì„ ë‹¤ì‹œ ì •ê·œí™”
                        if max(daily_pattern) > 0:
                            daily_pattern = daily_pattern / max(daily_pattern)
                            
                        X_5day.append(daily_pattern)
                        y_5day.append(pattern)
        
        return np.array(X_5day), np.array(y_5day)
    
    def _evaluate_models(self, X_5day, y_5day, patterns):
        """ëª¨ë¸ í‰ê°€"""
        predicted_labels = []
        reconstruction_errors = []

        for sample in X_5day:
            pred, recon, mse = self.predict(sample)
            predicted_labels.append(pred)
            reconstruction_errors.append(mse)

        predicted_labels = np.array(predicted_labels)

        from collections import Counter

        print("\nğŸ“Š íŒ¨í„´ë³„ ì˜ˆì¸¡ ê²°ê³¼:")
        for pattern in patterns:
            idxs = np.where(y_5day == pattern)[0]
            if len(idxs) == 0:
                continue
                
            total = len(idxs)
            pred_counts = Counter(predicted_labels[idxs])
            unknown_count = pred_counts.get('Unknown', 0)
            correct_count = pred_counts.get(pattern, 0)
            acc = correct_count / total
            unknown_ratio = unknown_count / total

            print(f"íŒ¨í„´ {pattern} â–¶ ì •í™•ë„: {acc:.2%}, ë¯¸í™•ì¸: {unknown_ratio:.2%}")

        # Unknown ì œì™¸ í˜¼ë™ í–‰ë ¬
        valid_mask = predicted_labels != 'Unknown'
        if sum(valid_mask) > 0:
            valid_patterns = [p for p in patterns if p in self.models]
            cm = confusion_matrix(y_5day[valid_mask], predicted_labels[valid_mask], labels=valid_patterns)
            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=valid_patterns)
            
            # í˜¼ë™ í–‰ë ¬ ì €ì¥
            plt.figure(figsize=(8, 6))
            disp.plot(cmap='Blues')
            plt.title("í˜¼ë™ í–‰ë ¬ (Unknown ì œì™¸)")
            plt.grid(False)
            plt.savefig(os.path.join(self.model_path, 'confusion_matrix.png'))
            plt.close()
    
    def _save_models(self):
        """ëª¨ë¸ ë° ì„ê³„ê°’ ì €ì¥"""
        for pattern, model in self.models.items():
            model_path = os.path.join(self.model_path, f'autoencoder_{pattern}.pt')
            torch.save(model.state_dict(), model_path)
            print(f"âœ… íŒ¨í„´ {pattern} ì˜¤í† ì¸ì½”ë” ì €ì¥ ì™„ë£Œ: {model_path}")

        threshold_path = os.path.join(self.model_path, 'thresholds.pkl')
        with open(threshold_path, 'wb') as f:
            pickle.dump(self.thresholds, f)
            print(f"âœ… ì„ê³„ê°’ ì €ì¥ ì™„ë£Œ: {threshold_path}")
    
    def load_models(self):
        """ì €ì¥ëœ ëª¨ë¸ ë° ì„ê³„ê°’ ë¡œë“œ"""
        self.models = {}
        self.thresholds = {}
        
        # ì„ê³„ê°’ ë¡œë“œ
        threshold_path = os.path.join(self.model_path, 'thresholds.pkl')
        if os.path.exists(threshold_path):
            with open(threshold_path, 'rb') as f:
                self.thresholds = pickle.load(f)
        else:
            return {
                "status": "error",
                "error": "ì €ì¥ëœ ì„ê³„ê°’ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ëª¨ë¸ ë¡œë“œ
        models_loaded = 0
        for pattern in self.patterns:
            model_path = os.path.join(self.model_path, f'autoencoder_{pattern}.pt')
            if os.path.exists(model_path):
                model = EncoderModel()
                model.load_state_dict(torch.load(model_path))
                model.eval()
                self.models[pattern] = model
                models_loaded += 1
        
        if models_loaded == 0:
            return {
                "status": "error",
                "error": "ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            }
            
        return {
            "status": "success", 
            "message": f"{models_loaded}ê°œ íŒ¨í„´ ì˜¤í† ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ",
            "patterns_loaded": list(self.models.keys())
        }
    
    def predict(self, sample):
        """ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ íŒ¨í„´ ì˜ˆì¸¡ (ë‚´ë¶€ìš©)"""
        if not self.models:
            return "Unknown", None, 0
            
        errors = {}
        for p, model in self.models.items():
            model.eval()
            with torch.no_grad():
                x = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)
                recon = model(x).squeeze(0).numpy()
                mse = mean_squared_error(sample, recon)
                errors[p] = (mse, recon)

        best_p = min(errors, key=lambda k: errors[k][0])
        best_mse = errors[best_p][0]

        if best_mse > self.thresholds.get(best_p, float('inf')):
            return "Unknown", errors[best_p][1], best_mse
        else:
            return best_p, errors[best_p][1], best_mse
            
    def predict_pattern(self, data):
        """íŒ¨í„´ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ë©”ì„œë“œ"""
        # ë¨¼ì € ëª¨ë¸ ë¡œë“œ ì‹œë„
        if not self.models:
            load_result = self.load_models()
            if load_result["status"] == "error":
                return load_result
            
        # ë°ì´í„° ì „ì²˜ë¦¬
        try:
            # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì…ë ¥ëœë‹¤ê³  ê°€ì •
            # í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì „ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
            pred_pattern, _, confidence = self.predict(data)
            
            return {
                "status": "success",
                "pattern": pred_pattern,
                "confidence": 1.0 - min(1.0, confidence)  # ì˜¤ì°¨ë¥¼ ì‹ ë¢°ë„ë¡œ ë³€í™˜
            }
        except Exception as e:
            return {
                "status": "error",
                "error": f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    def _optimize_thresholds(self, X, y):
        """
        ì„ê³„ê°’ì„ ìµœì í™”í•˜ì—¬ Unknown ë¹„ìœ¨ì´ ëª©í‘œì¹˜ ì´í•˜ê°€ ë˜ë„ë¡ ì¡°ì •
        
        Args:
            X: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y: ì‹¤ì œ ë ˆì´ë¸”
            
        Returns:
            dict: ìµœì í™” ê²°ê³¼
        """
        if not self.models:
            self.pipeline_status['current_stage'] = 'threshold_optimization_failed'
            self.pipeline_status['last_error'] = "ìµœì í™”í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
            return {"status": "error", "error": "ìµœì í™”í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}
            
        # ì´ˆê¸° ì„ê³„ê°’ ì €ì¥ (ë¡¤ë°± ê°€ëŠ¥í•˜ë„ë¡)
        initial_thresholds = self.thresholds.copy()
        
        # ì„ê³„ê°’ ì¡°ì • ì‹œë„
        for retry in range(self.max_retries):
            # í˜„ì¬ ì„ê³„ê°’ìœ¼ë¡œ Unknown ë¹„ìœ¨ ê³„ì‚°
            unknown_count = 0
            
            for i in range(len(X)):
                pred, _, _ = self.predict(X[i])
                if pred == "Unknown":
                    unknown_count += 1
                    
            unknown_ratio = unknown_count / len(X)
            
            print(f"ì‹œë„ {retry+1}/{self.max_retries}: Unknown ë¹„ìœ¨ = {unknown_ratio:.2%}, ëª©í‘œ = {self.unknown_ratio_max:.2%}")
            
            # í˜„ì¬ ì„ê³„ê°’ ìƒíƒœ ê¸°ë¡
            self.pipeline_status['threshold_history'][f'attempt_{retry+1}'] = {
                'unknown_ratio': unknown_ratio,
                'thresholds': self.thresholds.copy()
            }
            
            # ëª©í‘œ ë‹¬ì„± - Unknown ë¹„ìœ¨ì´ ëª©í‘œ ì´í•˜
            if unknown_ratio <= self.unknown_ratio_max:
                self.pipeline_status['current_stage'] = 'threshold_optimization_success'
                return {
                    "status": "success",
                    "message": f"ì„ê³„ê°’ ìµœì í™” ì™„ë£Œ (ì‹œë„ {retry+1}/{self.max_retries})",
                    "unknown_ratio": unknown_ratio,
                    "thresholds": self.thresholds.copy()
                }
            
            # ì„ê³„ê°’ ì¡°ì • - ë‹¤ìŒ ë©€í‹°í”Œë¼ì´ì–´ ì‹œë„
            if retry < len(self.std_multipliers) - 1:
                next_multiplier = self.std_multipliers[retry + 1]
                
                # ëª¨ë“  íŒ¨í„´ì˜ ì„ê³„ê°’ ì¡°ì •
                for pattern in self.models.keys():
                    # ëª¨ë¸ í‰ê°€
                    model = self.models[pattern]
                    pattern_data = X[y == pattern]
                    
                    if len(pattern_data) > 0:
                        model.eval()
                        with torch.no_grad():
                            outputs = model(torch.tensor(pattern_data, dtype=torch.float32)).numpy()
                        mses = [mean_squared_error(x, y) for x, y in zip(pattern_data, outputs)]
                        mse_mean = np.mean(mses)
                        mse_std = np.std(mses)
                        
                        # ìƒˆ ì„ê³„ê°’ ì„¤ì •
                        self.thresholds[pattern] = mse_mean + next_multiplier * mse_std
                        print(f"íŒ¨í„´ {pattern} ì„ê³„ê°’ ì¡°ì •: {next_multiplier} * std")
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - íŒŒì´í”„ë¼ì¸ ì¡°ì • í•„ìš”
        self.pipeline_status['current_stage'] = 'threshold_optimization_failed'
        self.pipeline_status['needs_dbscan_adjustment'] = True
        
        # ì›ë˜ ì„ê³„ê°’ìœ¼ë¡œ ë³µì›
        self.thresholds = initial_thresholds.copy()
        
        return {
            "status": "error",
            "message": f"ìµœëŒ€ ì‹œë„ íšŸìˆ˜({self.max_retries})ë¥¼ ì´ˆê³¼í–ˆì§€ë§Œ ëª©í‘œ Unknown ë¹„ìœ¨({self.unknown_ratio_max:.2%})ì— ë„ë‹¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "unknown_ratio": unknown_ratio,
            "thresholds": self.thresholds.copy(),
            "recommendation": "DBSCAN íŒŒë¼ë¯¸í„° ì¬ì¡°ì • í•„ìš”"
        }
        
    def get_pipeline_status(self):
        """í˜„ì¬ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë°˜í™˜"""
        return self.pipeline_status