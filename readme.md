# GitLab Branch ìš´ì˜ ì „ëµ

## Voice ID ê¸°ë°˜ ì¶”ì²œ ì„œë¹„ìŠ¤ PJT Branch ìš´ì˜ ì „ëµ

í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” ë°ì´í„° ë¶„ì„ í”„ë¡œì íŠ¸ë¡œ  
1ì°¨ì ìœ¼ë¡œ ê°œì¸ í´ë¼ìš°ë“œ ë°±ì—…ì„ ëª©ì ìœ¼ë¡œ ë¸Œëœì¹˜ë¥¼ ìš´ì˜í•©ë‹ˆë‹¤.  

ë¶„ì„ì„ í†µí•´ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ê³  ì„œë¹„ìŠ¤ ê¸°íšì´ ì™„ë£Œë˜ë©´, Git flow ê¸°ë°˜ì˜ Branch ì „ëµì„ ì‚¬ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤.  

---

## [Branch ìš´ì˜ ì „ëµ]

### 0.1. ê°œì¸ Branch
- ë¡œì»¬ ê°œë°œ í™˜ê²½ ë° Colabì—ì„œ ë°ì´í„° ë¶„ì„ ì§„í–‰  
- ê°œì¸ ë°±ì—…ìš© ë¸Œëœì¹˜ëŠ” ì‚¬ìš©ì ì´ë¦„ìœ¼ë¡œ ë„¤ì´ë° (`backup/<your_name>`)  
- ê°œì¸ BranchëŠ” `main` ë˜ëŠ” `dev` Branch ë“±ê³¼ ë³‘í•©ë˜ì§€ ì•Šìœ¼ë©°, ë°±ì—… ìš©ë„ë¡œë§Œ ì‚¬ìš©  

### 0.2. ê°œì¸ í´ë¼ìš°ë“œ ë°±ì—… - Google Drive ì—°ë™
- GitLabê³¼ Google Driveë¥¼ ì—°ë™í•˜ì—¬ ë³€ê²½ ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ë°±ì—…  

### 0.3. Merge ì •ì±…
- ê°œì¸ ë¸Œëœì¹˜ëŠ” ë³‘í•© ëŒ€ìƒì´ ì•„ë‹ˆë©°, ê¸°ëŠ¥ì´ ì™„ì„±ë˜ë©´ `dev` ë˜ëŠ” `feature/xxx` ë¸Œëœì¹˜ì—ì„œ ì •ì‹ PRì„ ìƒì„±  
- ê°œì¸ ë¸Œëœì¹˜ëŠ” ì¼ì • ê¸°ê°„ ìœ ì§€í•˜ë©° ë°±ì—… ìš©ë„ë¡œ í™œìš©  

### 0.4. ì„œë¹„ìŠ¤ ê°œë°œ ë‹¨ê³„ì—ì„œì˜ Branch ìš´ì˜
- ì´í›„ ì„œë¹„ìŠ¤ ê°œë°œì´ ì§„í–‰ë˜ë©´ `main` ë° `dev` ë¸Œëœì¹˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ ê³µì‹ì ì¸ ë°°í¬ ë° ê°œë°œì„ ê´€ë¦¬  
- `feature` ë¸Œëœì¹˜ë¥¼ í™œìš©í•˜ì—¬ ê°œë³„ ê¸°ëŠ¥ì„ ê°œë°œí•œ í›„, `dev` ë¸Œëœì¹˜ì— ë³‘í•©  
- ì•ˆì •ì ì¸ ë°°í¬ë¥¼ ìœ„í•´ `main` ë¸Œëœì¹˜ëŠ” ìµœì¢… ê²€ì¦ëœ ì½”ë“œë§Œ ë°˜ì˜


# 1. ì›ë³¸ ë°ì´í„° EDA

### 1.1 ë°ì´í„° ì„¤ëª…

ë¶„ì„ì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ : `water_purifier_event_history.csv`

3ì¼ì¹˜ event history ë°ì´í„°

- `uuid`: ì‚¬ìš©ì ê³ ìœ  ID
    - uuid ì¤‘ a, bëŠ” í•œ ê¸°ê¸°ì—ì„œ 2ëª…ì˜ ì‚¬ìš©ìë¥¼ êµ¬ë³„í•˜ê¸° ìœ„í•œ uuid
- `eventTime`: ì¶œìˆ˜ ì‹œê°„
- `desiredcapacity`: ì¶œìˆ˜ëŸ‰(ml)
- `desiredtype`: ìš”ì²­ëœ ë¬¼ì˜ ì˜¨ë„ ìœ í˜• (hotwater, ambientwater, coldwater)

### Water Usage Data

| id | uuid | eventTime | desiredCapacity | desiredType |
|----|------|-----------|-----------------|-------------|
| 0 | 1-a | 2024-02-01 0:13 | 140 | ambientwater |
| 1 | 1-a | 2024-02-01 4:05 | 1000 | ambientwater |
| 2 | 1-a | 2024-02-01 8:02 | 120 | hotwater |
| 3 | 1-a | 2024-02-01 8:29 | 120 | coldwater |
| 4 | 1-a | 2024-02-01 9:11 | 140 | hotwater |
| ... | ... | ... | ... | ... |
| 999995 | 999-b | 2024-02-03 19:44 | 260 | hotwater |
| 999996 | 999-b | 2024-02-03 19:52 | 500 | coldwater |
| 999997 | 999-b | 2024-02-03 20:24 | 120 | hotwater |
| 999998 | 999-b | 2024-02-03 20:52 | 260 | ambientwater |
| 999999 | 999-b | 2024-02-03 21:27 | 120 | ambientwater |


### DataFrame Information

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 4 columns):
 #   Column           Non-Null Count    Dtype  
---  ------           --------------    -----  
 0   uuid             1000000 non-null  object 
 1   eventTime        1000000 non-null  object 
 2   desiredcapacity  1000000 non-null  int64  
 3   desiredtype      1000000 non-null  object 
dtypes: int64(1), object(3)
memory usage: 30.5+ MB
```


### 1.2 ë¬¼ ìš©ëŸ‰ ë¶„ì„

ë¬¼ ìš©ëŸ‰(desiredcapacity) íŒŒì´ ì°¨íŠ¸ë¡œ ì‹œê°í™”

![alt text](assets/WaterCapacity.png)

### 1.3 ë¬¼ ì˜¨ë„ ì„ í˜¸ë„ ë¶„ì„

ë¬¼ ì˜¨ë„(desiredtype) ì„ í˜¸ë„ íŒŒì´ ì°¨íŠ¸ë¡œ ì‹œê°í™”

![alt text](</assets/waterTemperature.png>)


### 1.4 ì‹œê°„ë³„ ë¬¼ ì‚¬ìš© íŒ¨í„´ ë¶„ì„

ì •ìˆ˜ê¸° ì‚¬ìš©ëŸ‰ì„ ì‹œê°„ëŒ€ë³„ë¡œ ë¶„ì„í•˜ì—¬ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì‚¬ìš© íŒ¨í„´ ì‹œê°í™”


#### 1.4.1 ì‹œê°„ë³„ ì „ì²´ ì‚¬ìš© ìˆ˜ (1ì‹œê°„ ë‹¨ìœ„)

![ì‹œê°„ë³„ ì „ì²´ ì‚¬ìš© ìˆ˜ (1ì‹œê°„ ë‹¨ìœ„).png](<assets/TotalWaterUsageCount(1hour).png>)

```python
# 'eventTime'ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
data['eventTime'] = pd.to_datetime(data['eventTime'])

# 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì „ì²´ ì‚¬ìš© ìˆ˜ ê³„ì‚°
total_usage = data.set_index('eventTime').resample('1h').size()

# Xì¶• ë¼ë²¨ í˜•ì‹ ë³€ê²½ (ì¼-ì‹œ í˜•íƒœë¡œ ë³€í™˜)
formatted_labels = [f"{d.day}-{d.hour}" for d in total_usage.index]

# ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„± (ì „ì²´ ì‚¬ìš© ìˆ˜)
plt.figure(figsize=(12, 6))
plt.bar(formatted_labels, total_usage.values, width=0.8, color='gray')
plt.ylabel("ì‚¬ìš© ìˆ˜")
plt.title("ì‹œê°„ë³„ ì „ì²´ ì‚¬ìš© ìˆ˜ (1ì‹œê°„ ë‹¨ìœ„)")
plt.xticks(rotation=45)
plt.show()
```

#### 1.4.2 ì˜¨ë„ë³„ ì‚¬ìš© ìˆ˜ ë¶„ì„

ê° ì˜¨ë„ íƒ€ì…ë³„ë¡œ ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´

##### ì˜¨ìˆ˜(Hot Water) ì‚¬ìš© íŒ¨í„´

![ì‹œê°„ë³„ ì˜¨ìˆ˜ ì‚¬ìš© ìˆ˜](</assets/HotWaterUsageCount(1hour).png>)

##### ë¯¸ì˜¨ìˆ˜(Ambient Water) ì‚¬ìš© íŒ¨í„´

![ì‹œê°„ë³„ ë¯¸ì˜¨ìˆ˜ ì‚¬ìš© ìˆ˜](<../assets/assets/AmbientwaterUsageCount(1hour).png>)

##### ëƒ‰ìˆ˜(Cold Water) ì‚¬ìš© íŒ¨í„´

![ì‹œê°„ë³„ ëƒ‰ìˆ˜ ì‚¬ìš© ìˆ˜](<../assets/ColdWaterUsageCount(1hour).png>)


#### 1.4.3 ì‹œê°„ë³„ ì˜¨ë„ ë¹„ìœ¨ ë¶„ì„

íŠ¹ì • ì‚¬ìš©ìì˜ ì‹œê°„ëŒ€ë³„ ì˜¨ë„ ì„ í˜¸ë„ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.

![ì‹œê°„ë³„ ì‚¬ìš© ìˆ˜ ë° ì˜¨ë„ë³„ ë¹„ìœ¨ - A](assets/UserCountAndTemperaturePercent(1hour)_1_a.png)
![ì‹œê°„ë³„ ì‚¬ìš© ìˆ˜ ë° ì˜¨ë„ë³„ ë¹„ìœ¨ - B](assets/UserCountAndTemperaturePercent(1hour)_1_b.png)


#### 1.4.4 ì‹œê°„ë³„ ì „ì²´ ì‚¬ìš©ëŸ‰ (1ì‹œê°„ ë‹¨ìœ„)



### 1.5 ë¬¼ ìš©ëŸ‰ ë° ì˜¨ë„ ì¡°í•© ë¶„ì„

ë¬¼ ìš©ëŸ‰ê³¼ ì˜¨ë„ì˜ ì¡°í•© ë¶„ì„ì„ í†µí•´ ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” ì¡°í•©ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤.
![ë¬¼ìœ í˜•ë°ì˜¨ë„](assets/WaterCapacityAndTemperature.png)


```python
# ë¬¼ ìš©ëŸ‰ ê¸°ì¤€ ì„¤ì • (ì •ë ¬ ìˆœì„œ)
capacity_order = ['120', '140', '260', '500', '1000']

# 'desiredcapacity'ì™€ 'desiredtype'ì„ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ ë²”ì£¼ ìƒì„±
data['combined'] = data['desiredcapacity'].astype(str) + " - " + data['desiredtype']

# ê° ì¡°í•©ì˜ ê°œìˆ˜ ì„¸ê¸°
combined_counts = data['combined'].value_counts()

# ì •ë ¬: ë¬¼ ìš©ëŸ‰ ìˆœì„œì— ë§ì¶° ì •ë ¬
sorted_combined_counts = sorted(
    combined_counts.items(), 
    key=lambda x: (capacity_order.index(x[0].split(" - ")[0]), x[0])
)

# ì •ë ¬ëœ ë°ì´í„°ë¥¼ ë¶„ë¦¬
sorted_labels = [x[0] for x in sorted_combined_counts]
sorted_values = [x[1] for x in sorted_combined_counts]

# ìƒ‰ìƒ ë§¤í•‘
bar_colors = [colors[label.split(" - ")[1]] for label in sorted_labels]

# ë§‰ëŒ€ ê·¸ë˜í”„ (ë¬¼ ìš©ëŸ‰ & ë¬¼ ì˜¨ë„ ì¡°í•©)
plt.figure(figsize=(12, 6))
plt.bar(sorted_labels, sorted_values, color=bar_colors)
plt.xlabel("ë¬¼ ìš©ëŸ‰ - ë¬¼ ì˜¨ë„ ì¡°í•©")
plt.ylabel("ìˆ˜ëŸ‰")
plt.title("ë¬¼ ìš©ëŸ‰ & ë¬¼ ì˜¨ë„ ì¡°í•© (Bar Chart)")
plt.xticks(rotation=45)
plt.show()
```

### 1.6 ì‚¬ìš©ìë³„ ì‚¬ìš©ëŸ‰ ë¶„ì„

ì‚¬ìš©ìë³„ ì´ ì‚¬ìš©ëŸ‰(desiredcapacity í•©ê³„)ê³¼ ì˜¨ë„ë³„ ì‚¬ìš©ëŸ‰ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.



### 1.7 ì „ì²´ ì‹œê°„ë³„ ì‚¬ìš©ëŸ‰ ë¶„ì„

ì „ì²´ ë° ì˜¨ë„ë³„ ì‹œê°„ëŒ€ë³„ ì‚¬ìš©ëŸ‰(desiredcapacity í•©ê³„)ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
![desiredcapacity](assets/TotalUsageperhour(1hour).png)


### 1.8 ì‚¬ìš©ìë³„ ì˜¨ë„ ì„ í˜¸ë„ ë¶„ì„

ê°œë³„ ì‚¬ìš©ìì˜ ì˜¨ë„ ì„ í˜¸ë„ë¥¼ íŒŒì´ ì°¨íŠ¸ë¡œ ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤.

### 1.9 ë‹¨ì‹œê°„ ë‚´ ì¬ìš”ì²­ íŒ¨í„´ ë¶„ì„

1ë¶„ ì´ë‚´ ì •ìˆ˜ê¸° ì¬ìš”ì²­ ì‹œ ë™ì¼í•œ ì˜¨ë„ë¥¼ ì„ íƒí•˜ëŠ”ì§€ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.


![ë‹¨ì‹œê°„ ë‚´ ì¬ìš”ì²­](assets/RerequestWaterPurifierIn1min.png)
```python
# eventTimeì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
data['eventTime'] = pd.to_datetime(data['eventTime'])

# ë‹¤ìŒ í–‰ê³¼ì˜ ì‹œê°„ ì°¨ì´ë¥¼ ë¶„ ë‹¨ìœ„ë¡œ ê³„ì‚°
data['timediff'] = (data['eventTime'].shift(-1) - data['eventTime']).dt.total_seconds() / 60

# ê·¸ë£¹ ë²ˆí˜¸ í• ë‹¹
data['group'] = 0
group_id = 0

# ê·¸ë£¹í™” ì§„í–‰
for i in range(len(data)):
    data.loc[i, 'group'] = group_id
    if data.loc[i, 'timediff'] > 1:
        group_id += 1

# ê·¸ë£¹ë³„ desiredtype ì¼ê´€ì„± í™•ì¸ í•¨ìˆ˜
def check_group_consistency(data):
    return data.groupby('group')['desiredtype'].nunique() == 1

# ì¼ê´€ì„± í™•ì¸ ë° íŒŒì´ ì°¨íŠ¸ ìƒì„±
group_consistency = check_group_consistency(data)
true_count = group_consistency.sum()
false_count = len(group_consistency) - true_count

plt.figure(figsize=(7, 7))
plt.pie(
    [true_count, false_count], 
    labels=['ì¼ê´€ë¨ (True)', 'ì¼ê´€ë˜ì§€ì•ŠìŒ (False)'], 
    autopct='%1.1f%%', 
    colors=['green', 'red'], 
    startangle=140, 
    wedgeprops={'edgecolor': 'black'}
)
plt.title("1ë¶„ë‚´ ì •ìˆ˜ê¸° ì¬ìš”ì²­ì´ ë˜‘ê°™ì€ ì˜¨ë„ë¥¼ í•˜ì˜€ëŠ”ê°€?")
plt.show()
```

## 2. ì£¼ìš” ë°œê²¬ì 

### 2.1 ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´

### 2.2 ì˜¨ë„ë³„ ì‚¬ìš© íŒ¨í„´


### 2.3 ì¶œìˆ˜ëŸ‰ ë¶„ì„


## 3. ê²°ë¡ 


**ì¶”í›„ ì‘ì„±**


# 2. ìƒì„± ë°ì´í„° EDA
## 2.1 ë°ì´í„° ìƒì„±

### ê³µí†µ íŒ¨í„´ ê°€ì •(ë…¸ì´ì¦ˆ ìƒì„± ë°©ë²•)

- í•˜ë£¨ ì¶œìˆ˜ëŸ‰ì„ 0.6~1.3Lì˜ ë¬¼ì„ ë§ˆì‹ ë‹¤ê³  ê°€ì •.
- ì£¼ íŒ¨í„´ì´ ë˜ëŠ” ì‹œê°„ì— ë‚˜ëˆ ì„œ 0.5Lì˜ ë¬¼ì„ ë§ˆì‹ ë‹¤ê³  ê°€ì •.

| ìš”ì²­ íšŸìˆ˜ | ì¶œìˆ˜ëŸ‰ ë²”ìœ„        |
|:--------:|:-----------------:|
| 1     | 0.45L ~ 0.5L  |
| 2     | 0.2L ~ 0.25L  |
| 3     | 0.15L ~ 0.2L  |
| 4     | 0.08L ~ 0.13L |
 
- ì£¼íŒ¨í„´ì—ì„œ ë¬¼ì˜ ìš”ì²­ íšŸìˆ˜ëŠ” ê±°ì˜ ë™ì¼í•˜ë‹¤ê³  ê°€ì •
- ì£¼ íŒ¨í„´ ì‹œê°„ì€ randomìœ¼ë¡œ ì‹œì‘ ì‹œê°„ê³¼ ë²”ìœ„ë¥¼ ì„¤ì •í•´ ê±°ê¸°ì—ì„œ níšŒ ìš”ì²­í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìƒì„±(30ë¶„ ë‹¨ìœ„)
- í•˜ë£¨ ìµœëŒ€ 8ë²ˆì˜ ë¬¼ì„ ë§ˆì‹ ë‹¤ê³  ê°€ì •.


### íŒ¨í„´ A ( ì €ë… ì‹œê°„ì— ë¬¼ì„ ë§ì´ ë§ˆì‹œëŠ” ì‚¬ëŒ)
- ì €ë… ì‹œê°„

| ì‹œì‘í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ìš”ì²­í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ì¶œìˆ˜ëŸ‰ ë²”ìœ„  |
|:--------:|:-----------------:|:-----------------:|
| 17ì‹œ ~ 18ì‹œ 30ë¶„ | 3ì‹œê°„ ~ 4ì‹œê°„ 30ë¶„ | 1íšŒ ~ 4íšŒ |

- ìƒì„± ì½”ë“œ
```python
start_hour2 = int(np.random.uniform(34, 38))
eating_hour2 = int(np.random.uniform(6, 10))
boost_hours = [list(range(start_hour2, start_hour2 + eating_hour2 + 1))]
count = [np.random.randint(1, 5)]

meta_info['start_hours'].append(start_hour2)
meta_info['eating_hours'].append(eating_hour2)
meta_info['count'] = count

waterForm = [(0, 0), (450, 500), (200, 250), (150, 200), (80, 130)]
for i in range(len(boost_hours)):
    count = fixed_count[i] if i < len(fixed_count) else 0
    if count == 0:
        continue
    times = np.random.choice(boost_hours[i], size=count, replace=False)
    for hour in times:
        usage[hour] += int(np.random.uniform(*waterForm[count])) // 10 * 10
max_drinks = 8 - sum(fixed_count)
base_hours = list(set(range(48)) - set().union(*boost_hours))
base_drinks = np.random.randint(min(3, max_drinks), max(3, max_drinks) + 1)
if base_hours:
    times = np.random.choice(base_hours, size=base_drinks, replace=False)
    for hour in times:
        usage[hour] += int(np.random.uniform(80.0, 130.0)) // 10 * 10
```

### íŒ¨í„´ B ( ì ì‹¬, ì €ë… ì‹œê°„ì— ë¬¼ì„ ë§ì´ ë§ˆì‹œëŠ” ì‚¬ëŒ)
- ì ì‹¬ ì‹œê°„

| ì‹œì‘í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ìš”ì²­í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ì¶œìˆ˜ëŸ‰ ë²”ìœ„  |
|---------------|---------------|---------|
| 10ì‹œ ~ 12ì‹œ 30ë¶„ | 3ì‹œê°„ ~ 4ì‹œê°„ 30ë¶„ | 1íšŒ ~ 4íšŒ |

- ì €ë… ì‹œê°„

| ì‹œì‘í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ìš”ì²­í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ì¶œìˆ˜ëŸ‰ ë²”ìœ„  |
|---------------|---------------|---------|
| 17ì‹œ ~ 18ì‹œ 30ë¶„ | 3ì‹œê°„ ~ 4ì‹œê°„ 30ë¶„ | 1íšŒ ~ 4íšŒ |

- ìƒì„± ì½”ë“œ
```python

start_hour1 = int(np.random.uniform(20, 26))
eating_hour1 = int(np.random.uniform(6, 10))
start_hour2 = int(np.random.uniform(34, 38))
eating_hour2 = int(np.random.uniform(6, 10))

boost_hours = [
    list(range(start_hour1, start_hour1 + eating_hour1 + 1)),
    list(range(start_hour2, start_hour2 + eating_hour2 + 1))
]
x = np.random.randint(1, 5)
if x != 4:
    count = [x, np.random.randint(1, 5)]
else:
    count = [x, np.random.randint(1, 4)]

meta_info['start_hours'] = [start_hour1, start_hour2]
meta_info['eating_hours'] = [eating_hour1, eating_hour2]
meta_info['count'] = count

waterForm = [(0, 0), (450, 500), (200, 250), (150, 200), (80, 130)]
for i in range(len(boost_hours)):
    count = fixed_count[i] if i < len(fixed_count) else 0
    if count == 0:
        continue
    times = np.random.choice(boost_hours[i], size=count, replace=False)
    for hour in times:
        usage[hour] += int(np.random.uniform(*waterForm[count])) // 10 * 10
max_drinks = 8 - sum(fixed_count)
base_hours = list(set(range(48)) - set().union(*boost_hours))
base_drinks = np.random.randint(min(3, max_drinks), max(3, max_drinks) + 1)
if base_hours:
    times = np.random.choice(base_hours, size=base_drinks, replace=False)
    for hour in times:
        usage[hour] += int(np.random.uniform(80.0, 130.0)) // 10 * 10
```

### íŒ¨í„´ C ( ì ì‹¬ ì‹œê°„ì— ë¬¼ì„ ë§ì´ ë§ˆì‹œëŠ” ì‚¬ëŒ)
- ì ì‹¬ ì‹œê°„

  | ì‹œì‘í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ìš”ì²­í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ì¶œìˆ˜ëŸ‰ ë²”ìœ„  |
  |---------------|---------------|---------|
  | 10ì‹œ ~ 12ì‹œ 30ë¶„ | 3ì‹œê°„ ~ 4ì‹œê°„ 30ë¶„ | 1íšŒ ~ 4íšŒ |

- ìƒì„± ì½”ë“œ
```python

start_hour1 = int(np.random.uniform(20, 26))
eating_hour1 = int(np.random.uniform(6, 10))
boost_hours = [list(range(start_hour1, start_hour1 + eating_hour1 + 1))]
count = [np.random.randint(1, 5)]

meta_info['start_hours'].append(start_hour1)
meta_info['eating_hours'].append(eating_hour1)
meta_info['count'] = count

waterForm = [(0, 0), (450, 500), (200, 250), (150, 200), (80, 130)]
for i in range(len(boost_hours)):
    count = fixed_count[i] if i < len(fixed_count) else 0
    if count == 0:
        continue
    times = np.random.choice(boost_hours[i], size=count, replace=False)
    for hour in times:
        usage[hour] += int(np.random.uniform(*waterForm[count])) // 10 * 10
max_drinks = 8 - sum(fixed_count)
base_hours = list(set(range(48)) - set().union(*boost_hours))
base_drinks = np.random.randint(min(3, max_drinks), max(3, max_drinks) + 1)
if base_hours:
    times = np.random.choice(base_hours, size=base_drinks, replace=False)
    for hour in times:
        usage[hour] += int(np.random.uniform(80.0, 130.0)) // 10 * 10
```

### íŒ¨í„´ D ( ì•„ì¹¨ ì‹œê°„ì— ë¬¼ì„ ë§ì´ ë§ˆì‹œëŠ” ì‚¬ëŒ)- ì‹ ê·œ íŒ¨í„´
- ì ì‹¬ ì‹œê°„

  | ì‹œì‘í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ìš”ì²­í•˜ëŠ” ì‹œê°„ ë²”ìœ„    | ì¶œìˆ˜ëŸ‰ ë²”ìœ„  |
  |---------------|---------------|---------|
  | 5ì‹œ ~ 7ì‹œ 30ë¶„ | 3ì‹œê°„ ~ 4ì‹œê°„ 30ë¶„ | 1íšŒ ~ 4íšŒ |

- ìƒì„± ì½”ë“œ
```python
usage = np.zeros(48)
start_hour3 = int(np.random.uniform(10, 16))
eating_hour3 = int(np.random.uniform(6, 10))
meta_info = {
    'count': [count],
    'start_hours': [],
    'eating_hours': []
}

if 'D' in pattern_type:
    boost_hours = [list(range(start_hour3, start_hour3 + eating_hour3 + 1))]

waterForm = [(0, 0), (450, 500), (200, 250), (150, 200), (80, 130)]

for i in range(len(boost_hours)):
    pattern_drink_times = np.random.choice(boost_hours[i], size=count, replace=False)
    for hour in pattern_drink_times:

        usage[hour] += int(np.random.uniform(waterForm[count][0], waterForm[count][1])) // 10 * 10

max_drinks = 8 - count
base_hours = list(set(range(48)) - set(boost_hours[0]))
base_drinks = np.random.randint(min(3, max_drinks), max(3, max_drinks) + 1)
drink_times = np.random.choice(base_hours, size=base_drinks, replace=False)

for hour in drink_times:
    drink_usage = int(np.random.uniform(80.0, 130.0)) // 10 * 10
    usage[hour] += drink_usage
meta_info['start_hours'].append(start_hour3)
meta_info['eating_hours'].append(eating_hour3)
```

## 2.2 ë°ì´í„° ì²˜ë¦¬(A, B, C)
### ë°ì´í„° ì²˜ë¦¬ ë°©ë²•
1. DBSCANì„ í™œìš©í•œ í´ëŸ¬ìŠ¤í„°ë§, Silhouette Scoreì„ í™œìš©í•œ ì‘ì§‘ë„ í™•ì¸ ë° ëŒ€í‘œ íŒ¨í„´ ì°¾ê¸°
2. DTW(Dynamic Time Warping)ë¥¼ í˜¸ë¼ìš©í•œ ìœ ì‚¬ë„ ì¸¡ì •
  - ì‹œê²Œì—´ê°„ ë°ì´í„° ê°„ì˜ ìœ ì‚¬ì„±ì„ ë¹„êµí•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜
  - ë°ì´í„° ê°„ì˜ ì†ë„ë‚˜ ê¸¸ì´ê°€ ë‹¬ë¼ë„ ì´ê²ƒì„ ê³ ë ¤í•˜ì—¬ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ê¸° ë•Œë¬¸ì— ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ì— ë§ì´ í™œìš©
  - ì‹œê°„ ì¶• ìƒìœ¼ë¡œ í¬ì¸íŠ¸ì˜ ìœ„ì¹˜ëŠ” ë‹¤ë¥´ì§€ë§Œ ì•ì´ë‚˜ ë’¤ì— ë°€ë ¤ìˆê³  ì‚¬ì´ì¦ˆê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°, ë°ì´í„°ê°„ì˜ ì‹œê³„ì—´ì„ ë¹„í‹€ì–´ ìœ ì‚¬ë„ë¥¼ ì¸¡ì • 

![img.png](assets/DTW.png)

3. IQR í™œìš©í•œ ë°ì´í„° ì´ìƒì¹˜ ì œê±°

```python
# í•˜ë£¨ ë‹¨ìœ„ ë²¡í„° ìƒì„±
daily_vectors = []
for (pid, date), group in df.groupby(['person_id', 'date']):
    sorted_group = group.sort_values(by=['hour', 'minute'])
    vec = sorted_group['amount'].values
    if len(vec) == 48:
        daily_vectors.append(vec)
daily_vectors = np.array(daily_vectors)

if len(daily_vectors) == 0:
    print(f"[{label}] No valid daily vectors.")
    return

# ì •ê·œí™”
daily_vectors_norm = normalize_each_day_minmax(daily_vectors)

# DBSCAN êµ°ì§‘í™”
dbscan = DBSCAN(eps=0.9, min_samples=3)

labels = dbscan.fit_predict(daily_vectors_norm)

# PCA ì‹œê°í™”ë¥¼ ìœ„í•œ ë³€í™˜
pca = PCA(n_components=2)
pca_result = pca.fit_transform(daily_vectors_norm)

# ìœ íš¨ ë²¡í„°ë§Œ (ë…¸ì´ì¦ˆ ì œì™¸)
valid_indices = labels != -1
valid_vectors = daily_vectors_norm[valid_indices]
valid_labels = labels[valid_indices]

if len(valid_vectors) == 0:
    print(f"[{label}] No clusters found.")
    return

# ------------------- (1) Silhouette Score ê³„ì‚° -------------------
sil_samples = silhouette_samples(valid_vectors, valid_labels)
sil_scores = {
    cluster_id: np.mean(sil_samples[valid_labels == cluster_id])
    for cluster_id in np.unique(valid_labels)
}

# í‘œë¡œ ì¶œë ¥
sil_table = pd.DataFrame({
    'Cluster': list(sil_scores.keys()),
    'Silhouette Score': list(sil_scores.values())
}).sort_values(by='Silhouette Score', ascending=False)

print(f"\n[{label}] Cluster-wise Silhouette Scores:")
display(sil_table)

# ------------------- (2) ê°€ì¥ ì‘ì§‘ë„ ë†’ì€ í´ëŸ¬ìŠ¤í„° â†’ ëŒ€í‘œ ë²¡í„° -------------------
best_cluster = sil_table.iloc[0]['Cluster']
best_vectors = valid_vectors[valid_labels == best_cluster]

# ê°€ì¥ ëŒ€í‘œì ì¸ ì‹¤ì œ ë²¡í„° ì°¾ê¸°
dtw_matrix = np.zeros((len(best_vectors), len(best_vectors)))
for i in range(len(best_vectors)):
    for j in range(i + 1, len(best_vectors)):
        dist = dtw.distance(best_vectors[i], best_vectors[j])
        dtw_matrix[i, j] = dist
        dtw_matrix[j, i] = dist
avg_dtw = dtw_matrix.mean(axis=1)
center_idx = np.argmin(avg_dtw)
center_vector = best_vectors[center_idx]

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ DTW ê³„ì‚°
cum_center = cumulative_minmax(center_vector)
dtw_cum_distances = [dtw.distance(cum_center, cumulative_minmax(vec)) for vec in daily_vectors_norm]
# ì¼ë°˜ íŒ¨í„´ìœ¼ë¡œ DTWí•´ë³¸ ê²°ê³¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì €ì¥
dtw_plain_distances = [dtw.distance(center_vector, vec) for vec in daily_vectors_norm]

# ------------------- ì´ìƒì¹˜ ì œê±° (IQR ë°©ì‹) -------------------
q1 = np.percentile(dtw_cum_distances, 25)
q3 = np.percentile(dtw_cum_distances, 75)
iqr = q3 - q1
upper_bound = q3 + 0.5 * iqr

# ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ë° ì¸ë±ìŠ¤ ì¶”ì¶œ
outlier_mask = np.array(dtw_cum_distances) > upper_bound

# ------------------- ì´ìƒì¹˜ ì œê±° (ìƒìœ„ 5% ì œê±° ë°©ì‹) -------------------

# threshold = np.percentile(dtw_cum_distances, 95)
# outlier_mask = np.array(dtw_cum_distances) > threshold

# ------------------- (3-2) ì „ì²´ ì  + DTW ì´ìƒì¹˜ ì‹œê°í™” -------------------
outlier_indices = np.where(outlier_mask)[0]
non_outlier_indices = np.where(~outlier_mask)[0]
# ì´ìƒì¹˜ ì œì™¸í•œ ë°ì´í„° ì¶”ì¶œ
filtered_vectors = daily_vectors[non_outlier_indices]  # ì›ë³¸ ë²¡í„° ì‚¬ìš© (ì •ê·œí™” X)
person_day_info = []
# ê° ë²¡í„°ê°€ ì–´ë–¤ ì‚¬ëŒ/ë‚ ì§œì— í•´ë‹¹í•˜ëŠ”ì§€ ë§¤ì¹­
idx = 0
for (pid, date), group in df.groupby(['person_id', 'date']):
if len(group) == 48:
if idx in non_outlier_indices:
start_hours = group['start_hours'].iloc[0]
eating_hours = group['eating_hours'].iloc[0]
counts = group['counts'].iloc[0]
person_day_info.append((pid, date, start_hours, eating_hours, counts))
idx += 1
```

### A ì´ìƒì¹˜ ì²˜ë¦¬
1. DBSCANì„ í™œìš©í•œ í´ëŸ¬ìŠ¤í„°ë§, Silhouette Scoreì„ í™œìš©í•œ ì‘ì§‘ë„ í™•ì¸ ë° ëŒ€í‘œ íŒ¨í„´ ì°¾ê¸°

- ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´

| Cluster | Silhouette Score |
|:--------:|:-----------------:|
| 35      | 	0.583731        |
| 24      | 	0.583714        |
| 8       | 	0.520445        |
| 0       | 	0.503378        |
| 9       | 	0.499320        |
| ...     | 	...             |
| 27      | 	0.031499        |
| 52      | 	0.017806        |
| 20      | 	0.013090        |
| 25      | 	0.009538        |
| 37      | 	-0.000248       |

- ëŒ€í‘œ íŒ¨í„´

![img.png](assets/RepresentativePatternA.png)

2. ëˆ„ì í•©ìœ¼ë¡œ DTWì™€ ì‹œí€€ìŠ¤ ê·¸ëŒ€ë¡œ DTWë¥¼ ê³„ì‚°í•œ ê²°ê³¼

![img.png](assets/DTWA.png)
3. IQR í™œìš©í•œ ë°ì´í„° ì´ìƒì¹˜ ì œê±°


### B ì´ìƒì¹˜ ì²˜ë¦¬
1. DBSCANì„ í™œìš©í•œ í´ëŸ¬ìŠ¤í„°ë§, Silhouette Scoreì„ í™œìš©í•œ ì‘ì§‘ë„ í™•ì¸ ë° ëŒ€í‘œ íŒ¨í„´ ì°¾ê¸°

- ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´

| Cluster | Silhouette Score |
|---------|------------------|
|4|	0.538214|
|5|	0.526342|
|6|	0.505096|
|3|	0.496367|
|1|	0.474208|
|2|	0.472166|
|0|	-0.033806|

- ëŒ€í‘œ íŒ¨í„´
![img.png](assets/RepresentativePatternB.png)

2. ëˆ„ì í•©ìœ¼ë¡œ DTWì™€ ì‹œí€€ìŠ¤ ê·¸ëŒ€ë¡œ DTWë¥¼ ê³„ì‚°í•œ ê²°ê³¼
![img.png](assets/DTWB.png)

3. IQR í™œìš©í•œ ë°ì´í„° ì´ìƒì¹˜ ì œê±°


### C ì´ìƒì¹˜ ì²˜ë¦¬
1. DBSCANì„ í™œìš©í•œ í´ëŸ¬ìŠ¤í„°ë§, Silhouette Scoreì„ í™œìš©í•œ ì‘ì§‘ë„ í™•ì¸ ë° ëŒ€í‘œ íŒ¨í„´ ì°¾ê¸°

- ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´

| Cluster | Silhouette Score |
|---------|------------------|
|24|	0.577219|
|15|	0.559158|
|12|	0.517747|
|8|	0.506237|
|20	|0.499416|
|...|	...|
|19|	0.058651|
|23|	0.043572|
|57|	0.033175|
|28|	-0.000347|
|32|	-0.036647|


- ëŒ€í‘œ íŒ¨í„´
![img.png](assets/RepresentativePatternC.png)

2. ëˆ„ì í•©ìœ¼ë¡œ DTWì™€ ì‹œí€€ìŠ¤ ê·¸ëŒ€ë¡œ DTWë¥¼ ê³„ì‚°í•œ ê²°ê³¼
![img.png](assets/DTWC.png)
3. IQR í™œìš©í•œ ë°ì´í„° ì´ìƒì¹˜ ì œê±°


# 3. ìƒì„± ë°ì´í„° ML ëª¨ë¸ í•™ìŠµ(ë¯¼ì² )
## 3.1 íŒ¨í„´ ë¶„ë¥˜ê¸° ìƒì„±
### 3.1.1 ì´ˆê¸° ë¶„ë¥˜ ëª¨ë¸ ìƒì„±(A,B,C ë¶„ë¥˜ê¸°)
1. ë¶„ë¥˜ê¸° ëª¨ë¸(AutoEncoder)
- ê° íŒ¨í„´ ë³„ AE ëª¨ë¸ ìƒì„± ë° `ë°ì´í„° í‰ê·  + 1,5 * í‘œì¤€í¸ì°¨`ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ í•´ì„œ 87%ì˜ ë°ì´í„° ì•ˆì— ë“¤ì–´ì˜¤ë©´ í•´ë‹¹ ëª¨ë¸ë¡œ ë¶„ë¥˜
- 48ì°¨ì›(24ì‹œê°„ 30ë¶„ ë‹¨ìœ„)-> (nnëª¨ë¸) -> 24ì°¨ì›-> RelU -> (nnëª¨ë¸) -> 12ì°¨ì› -> (nnëª¨ë¸) -> RelU -> 24ì°¨ì› -> (nnëª¨ë¸)-> 48ì°¨ì›

2. 5ì¼ì¹˜ ëˆ„ì í•©ì˜ normalizationí•œ ê°’ì„ ì´ìš©í•´ AE í•™ìŠµ

3. í•™ìŠµ ê²°ê³¼
Pattern A â–¶ Accuracy: 87.73%, Unknown: 8.95%
Pattern B â–¶ Accuracy: 88.20%, Unknown: 9.18%
Pattern C â–¶ Accuracy: 90.03%, Unknown: 8.62%

![img.png](assets/Classifier.png)

```python
# 2. Define Autoencoder
class Autoencoder(nn.Module):
    def __init__(self, input_dim=48, latent_dim=12):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 24),
            nn.ReLU(),
            nn.Linear(24, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 24),
            nn.ReLU(),
            nn.Linear(24, input_dim)
        )

    def forward(self, x):
        return self.decoder(self.encoder(x))

# 3. Train AE per pattern
def train_autoencoder(model, data, epochs=150, lr=1e-3):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    model.train()
    inputs = torch.tensor(data, dtype=torch.float32)

    for _ in range(epochs):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, inputs)
        loss.backward()
        optimizer.step()

patterns = ['A', 'B', 'C']
models = {}
thresholds = {}

for pattern in patterns:
    print(f"Training AE for Pattern {pattern}")
    data = X_5day[y_5day == pattern]
    model = Autoencoder()
    train_autoencoder(model, data)
    models[pattern] = model

    model.eval()
    with torch.no_grad():
        outputs = model(torch.tensor(data, dtype=torch.float32)).numpy()
    mses = [mean_squared_error(x, y) for x, y in zip(data, outputs)]
    thresholds[pattern] = np.mean(mses) + 1.5 * np.std(mses) # 1.5 87%, 2 90%

# 4. Prediction function
def predict(sample):
    errors = {}
    for p, model in models.items():
        model.eval()
        with torch.no_grad():
            x = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)
            recon = model(x).squeeze(0).numpy()
            mse = mean_squared_error(sample, recon)
            errors[p] = (mse, recon)

    best_p = min(errors, key=lambda k: errors[k][0])
    best_mse = errors[best_p][0]

    if best_mse > thresholds[best_p]:
        return "Unknown", errors[best_p][1], best_mse
    else:
        return best_p, errors[best_p][1], best_mse


predicted_labels = []
reconstruction_errors = []

for sample in X_5day:
    pred, recon, mse = predict(sample)
    predicted_labels.append(pred)
    reconstruction_errors.append(mse)

predicted_labels = np.array(predicted_labels)

from collections import Counter

print("\nğŸ“Š íŒ¨í„´ë³„ ì˜ˆì¸¡ ê²°ê³¼:")
for pattern in patterns:
    idxs = np.where(y_5day == pattern)[0]
    total = len(idxs)
    pred_counts = Counter(predicted_labels[idxs])
    unknown_count = pred_counts.get('Unknown', 0)
    correct_count = pred_counts.get(pattern, 0)
    acc = correct_count / total
    unknown_ratio = unknown_count / total

    print(f"Pattern {pattern} â–¶ Accuracy: {acc:.2%}, Unknown: {unknown_ratio:.2%}")
```

### 3.1.2 ì‹ ê·œ íŒ¨í„´ ì¸ì‹(A,B,C ë¶„ë¥˜ê¸°)
1. D íŒ¨í„´ í…ŒìŠ¤íŠ¸
```
ì˜ˆì¸¡ëœ íŒ¨í„´: Unknown
MSE: 0.08354 
Threshold ê¸°ì¤€: ì´ìƒ íƒì§€ë¨ (Unknown)
```
2. ì‹ ê·œ D AE ëª¨ë¸ ë° ThreshHold ì €ì¥
- 1000ëª… 30ì¼ -> ì´ 3000ê°œì˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´, AE ëª¨ë¸ ìƒì„± ê°€ëŠ¥í•˜ë‹¤.

![img.png](assets/ClassifierWithD.png)

## 3.2 íŒ¨í„´ë³„ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ìƒì„±

### 3.2.1 íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ë° í‰ê°€ ë°©ë²•
- ëª¨ë¸: XGBoost
Gradient Boosting ê¸°ë°˜ì˜ íŠ¸ë¦¬ ì•™ìƒë¸” ëª¨ë¸ë¡œ, ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ê³¼ ì•ˆì •ì ì¸ í•™ìŠµ ì†ë„ë¥¼ ì œê³µí•¨.
ê°œë³„ íŠ¸ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œ ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨(ì”ì°¨)ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì í™”í•¨.
ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì •ê·œí™” ê¸°ëŠ¥ê³¼ ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ ê°€ëŠ¥í•´ ì‹¤ë¬´ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë¨.

- K-Fold êµì°¨ ê²€ì¦
ì „ì²´ ë°ì´í„°ë¥¼ Kê°œì˜ ë¶€ë¶„(fold)ë¡œ ë‚˜ëˆˆ ë’¤, K-1ê°œë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ë‚¨ì€ 1ê°œë¡œ ê²€ì¦ì„ ìˆ˜í–‰.
ì´ ê³¼ì •ì„ Kë²ˆ ë°˜ë³µí•˜ë©°, ëª¨ë“  foldì—ì„œ ì–»ì€ í‰ê°€ ì§€í‘œì˜ í‰ê· ì„ ìµœì¢… ì„±ëŠ¥ìœ¼ë¡œ ì‚¬ìš©í•¨.
ë°ì´í„°ì˜ í¸í–¥ì„ ì¤„ì´ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì•ˆì •ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŒ.
ì¼ë°˜ì ìœ¼ë¡œ K=5 ë˜ëŠ” K=10ì´ ìì£¼ ì‚¬ìš©ë¨.

-ì •í™•ë„ ì¸¡ì • ë°©ë²•

|       ì§€í‘œ	       |ì„¤ëª…|	ìˆ˜ì‹|
|:---------------:|:----:|:----:|
| RÂ² Score (ê²°ì •ê³„ìˆ˜) |	ëª¨ë¸ì´ ì¢…ì† ë³€ìˆ˜ì˜ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜ˆì¸¡ë ¥ì´ ë†’ìŒ.| 	$R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$| 
| RMSE (Root Mean Square Error)| 	ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì˜¤ì°¨ë¥¼ ì œê³±í•˜ì—¬ í‰ê· í•œ í›„ ì œê³±ê·¼ì„ ì·¨í•œ ê°’. ê°’ì´ ì‘ì„ìˆ˜ë¡ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë†’ìŒ.| 	$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$| 
| Accuracy (10% ê¸°ì¤€)	| ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ì˜ Â±10% ë²”ìœ„ ë‚´ì— ë“¤ì–´ì˜¤ëŠ” ë¹„ìœ¨. íšŒê·€ ë¬¸ì œì—ì„œì˜ ì •ë°€ë„ ê°œë…ìœ¼ë¡œ í™œìš©.| 	$Accuracy = \frac{\text{10% ì´ë‚´ ì˜ˆì¸¡ ìˆ˜}}{\text{ì „ì²´ ìƒ˜í”Œ ìˆ˜}}$| 

### 3.2.2 ì •ìˆ˜ê¸° ë°ì´í„° Feature ì •ì˜í‘œ

| Feature ì´ë¦„ | ë°ì´í„° íƒ€ì… | ìœ íš¨ ë²”ìœ„ | ì„¤ëª… |
|-------------|------------|----------|------|
| person_id | BIGINT UNSIGNED | - | ì‚¬ìš©ì ë²ˆí˜¸ |
| time | DATETIME | - | ì¶œìˆ˜ ì‹œê°„ |
| output_seq | INT | 1 ~ 100 | í•˜ë£¨ ë‚´ ì¶œìˆ˜ ìˆœì„œ |
| ratio_to_prev_day | FLOAT | 0 ~ 10 | í˜„ì¬ ì‹œì  ì´ì „ê¹Œì§€ì˜ ëˆ„ì  ì¶œìˆ˜ëŸ‰ / ì´ì „ë‚  ì´ ì¶œìˆ˜ëŸ‰ |
| ratio_prev_to_total | FLOAT | 0 ~ 10 | ì´ì „ë‚  í˜„ì¬ì‹œê°„ê¹Œì§€ ë§ˆì‹  ì–‘ / ì´ì „ë‚  ì´ ì¶œìˆ˜ëŸ‰ |
| time_diff_prev_outputs | TIME | 00:00 ~ 24:00 | í˜„ì¬ ì¶œìˆ˜ ì‹œê°„ - ì´ì „ ì¶œìˆ˜ ì‹œê°„ |
| prev_sum | INT | 0 ~ 10000 | í˜„ì¬ ì‹œì  ì´ì „ê¹Œì§€ì˜ ëˆ„ì  ì¶œìˆ˜ëŸ‰ |
| prev_day_mean | FLOAT | 0 ~ 10000 | ì´ì „ë‚  ì¶œìˆ˜ëŸ‰ì˜ í‰ê·  |
| prev_day_std | FLOAT | 0 ~ 10000 | ì´ì „ë‚  ì¶œìˆ˜ëŸ‰ì˜ í‘œì¤€í¸ì°¨ |
| prev_day_total | INT | 0 ~ 10000 | ì´ì „ë‚  ì¶œìˆ˜ëŸ‰ì˜ ì´ í•© |
| slope_prev_day_n_n_minus_1 | INT | -10000 ~ 10000 | ì´ì „ë‚  ê°™ì€ ìˆœì„œì˜ ì¶œìˆ˜ëŸ‰ê³¼ ê·¸ ì´ì „ ì¶œìˆ˜ëŸ‰ì˜ ì°¨ì´ |
| slope_prev_day_n_minus_1_n_minus_2 | INT | -10000 ~ 10000 | ì´ì „ë‚  ê°™ì€ ìˆœì„œì˜ ì¶œìˆ˜ëŸ‰ê³¼ ê·¸ ì´ì „ ì¶œìˆ˜ëŸ‰ì˜ ì°¨ì´ ì™€ ì´ì „ë‚  ê°™ì€ ìˆœì„œì˜ ì´ì „ ì¶œìˆ˜ëŸ‰ê³¼ ê·¸ ì´ì „ ì‹œì ì˜ ì¶œìˆ˜ëŸ‰ ì°¨ì´ |
| avg_change_rate | FLOAT | 0 ~ 10000 | ì´ì „ë‚  ê·¸ë˜í”„ì˜ ë³€í™”ëŸ‰ í‰ê·  |
| prev_output | INT | 0 ~ 10000 | í˜„ì¬ ì‹œì  ì´ì „ ë²ˆì§¸ì˜ ì¶œìˆ˜ëŸ‰ |
| prev_prev_output | INT | 0 ~ 10000 | í˜„ì¬ ì‹œì  ì´ì „ì˜ ì´ì „ ë²ˆì§¸ì˜ ì¶œìˆ˜ëŸ‰ |

### 3.2.3 BaseModel HyperParameter

```python
'tree_method': 'hist',
'device': 'cuda',
'objective': 'reg:squarederror',
'max_depth': 6,
'random_state': 42
```


### 3.2.4 í•™ìŠµ ì½”ë“œ
```python

# XGBoost ëª¨ë¸ í•™ìŠµ
xgb_dtrain_all = xgb.DMatrix(X_all.values, label=y_all.values, feature_names=features)
xgb_model_all = xgb.train(params, xgb_dtrain_all, num_boost_round=100)

# ëª¨ë¸ ì €ì¥
model_path_all = os.path.join(model_dir_2, 'all.model')
xgb_model_all.save_model(model_path_all)
print(f"ì „ì²´ ë°ì´í„° ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path_all}")

# íŒ¨í„´ A, B, C, D ëª¨ë‘ ì‚¬ìš©
patterns_2 = ['A', 'B', 'C', 'D']
df_2 = df[df['pattern'].isin(patterns_2)].copy()
df_cudf_2 = cudf.DataFrame.from_pandas(df_2)

print(f"ë¸”ë¡ 2 ë°ì´í„° í¬ê¸° (A,B,C,D íŒ¨í„´): {len(df_2)}")

# K-Fold êµì°¨ ê²€ì¦ - ê°„ì†Œí™”ëœ í‰ê°€
X_pd_all = df_2[features]
y_pd_all = df_2['amount']

kfold = KFold(n_splits=5, shuffle=True, random_state=42)
r2_scores_all = []
rmse_scores_all = []
accuracy_all = []  # 10% ì´ë‚´ ì˜¤ì°¨ ë¹„ìœ¨

for train_idx, test_idx in kfold.split(X_pd_all):
    X_train, X_test = X_pd_all.iloc[train_idx], X_pd_all.iloc[test_idx]
    y_train, y_test = y_pd_all.iloc[train_idx], y_pd_all.iloc[test_idx]

    # ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
    dtrain_cv = xgb.DMatrix(cp.asarray(X_train.values), label=cp.asarray(y_train.values))
    dtest_cv = xgb.DMatrix(cp.asarray(X_test.values))
    xgb_model_cv = xgb.train(params, dtrain_cv, num_boost_round=100)
    y_pred = cp.asnumpy(xgb_model_cv.predict(dtest_cv))

```
# 4. Voice ID 

## 4.0 Voice ID ë°ì´í„°

### 4.0.1ë°ì´í„° ìˆ˜ì§‘
- ë°ì´í„°ëŠ” AI Hubì—ì„œ ì œê³µí•˜ëŠ” í•œêµ­ì–´ í™”ì ì¸ì‹ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
(https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=537)

ìœ„ ë°ì´í„° ì…‹ ì¤‘ LabelText = 'ë‘ë¹„ë‘'ë¥¼ í‰ê·  10íšŒ ì •ë„ ë°œí™”í•œ ìŒì„± ë°ì´í„°ì…‹ì„ í™œìš©í•©ë‹ˆë‹¤.

## 4.1 íŠ¹ì§• ì¶”ì¶œ ë°©ë²•
### 4.1.1 MFCC ìŒì„± íŠ¹ì§• ì¶”ì¶œ
  - MFCCëŠ” ì˜¤ë””ì˜¤ ì‹ í˜¸ì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” featureë¡œ, ì†Œë¦¬ì˜ ê³ ìœ í•œ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜ì…ë‹ˆë‹¤.
  - ì£¼ë¡œ ìŒì„± ì¸ì‹, í™”ì ì¸ì‹, ìŒì„± í•©ì„±, ìŒì•… ì¥ë¥´ ë¶„ë¥˜ ë“± ì˜¤ë””ì˜¤ ë„ë©”ì¸ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
  - ë¨¼ì € MFCCë¥¼ ì‰½ê²Œ ì´í•´í•˜ê¸° ìœ„í•´ MFCCì˜ ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.
  - ê° ìŒì„± íŒŒì¼ì—ì„œ ì‹œì‘ ì‹œì ê³¼ ì¢…ë£Œ ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬´ìŒ êµ¬ê°„ì„ ì œê±°í•©ë‹ˆë‹¤.
![img.png](assets/MFCC.png)
  1. ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ í”„ë ˆì„ë³„(ë³´í†µ 20ms - 40ms)ë¡œ ë‚˜ëˆ„ì–´ FFTë¥¼ ì ìš©í•´ Spectrumì„ êµ¬í•œë‹¤.
  2. Spectrumì— Mel Filter Bankë¥¼ ì ìš©í•´ Mel Spectrumì„ êµ¬í•œë‹¤.
  3. Mel Spectrumì— Cepstral ë¶„ì„ì„ ì ìš©í•´ MFCCë¥¼ êµ¬í•œë‹¤.
  
- ì¶”ê°€ ë…¸ì´ì¦ˆ ì œê±°
 - ì‹ í˜¸ëŒ€ì¡ìŒë¹„(SNR)ê°€ 5dB ì´í•˜ì¸ ë°ì´í„°ëŠ” ì œì™¸í•©ë‹ˆë‹¤ (ì‹¤ì œ ì ìš© ì‹œ ì œê±°ëœ ìƒ˜í”Œì€ ê±°ì˜ ì—†ìŒ).

- MFCC íŠ¹ì§• ì¶”ì¶œ ì½”ë“œ
```python
import os
import librosa
import numpy as np
import h5py
import json
import re
import noisereduce as nr
import soundfile as sf

# âœ… ì„¤ì •
SAVE_WAV = True  # ì „ì²˜ë¦¬ëœ .wav íŒŒì¼ ì €ì¥ ì—¬ë¶€

# âœ… ë””ë ‰í† ë¦¬ ì„¤ì •
input_base_dir = "/content/drive/MyDrive/SLink_PJT/Data/voiceData/source"
label_base_dir = "/content/drive/MyDrive/SLink_PJT/Data/voiceData/label"
h5_output_base_dir = "/content/drive/MyDrive/SLink_PJT/Data/MFCC_noiserm_Features"
wav_output_base_dir = "/content/drive/MyDrive/SLink_PJT/Processed_WAV"
status_file = "/content/drive/MyDrive/SLink_PJT/Data/process_noiserm_MFCC.json"

# âœ… ì§„í–‰ ìƒíƒœ ë¡œë“œ (ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°)
def load_progress():
  if os.path.exists(status_file):
    with open(status_file, "r") as f:
      return json.load(f)
  return {}

# âœ… ì§„í–‰ ìƒíƒœ ì €ì¥
def save_progress(processed_files):
  with open(status_file, "w") as f:
    json.dump(processed_files, f, indent=4)

# âœ… ì²˜ë¦¬ëœ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
processed_files = load_progress()

# âœ… JSON ë°ì´í„° ë¡œë“œ
def load_label_data(file_path):
  label_path = file_path.replace(input_base_dir, label_base_dir).replace(".wav", ".json")
  if not os.path.exists(label_path):
    raise Exception(f"âš ï¸ ë¼ë²¨ë§ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {label_path}")

  with open(label_path, "r") as f:
    label_data = json.load(f)

  try:
    snr = int(label_data["Wav"]["SignalToNoiseRatio"].replace("dB", ""))
    speech_start = float(label_data["Other"]["SpeechStart"])
    speech_end = float(label_data["Other"]["SpeechEnd"])
    age_group = label_data["Speaker"]["Age"]
    noise_env = label_data["Environment"]["NoiseEnviron"]
    recording_env = label_data["Environment"]["RecordingEnviron"]
  except KeyError as e:
    raise Exception(f"âš ï¸ ë¼ë²¨ë§ ë°ì´í„° ì˜¤ë¥˜: {e} in {label_path}")

  return {
    "snr": snr,
    "speech_start": speech_start,
    "speech_end": speech_end,
    "age_group": age_group,
    "noise_env": noise_env,
    "recording_env": recording_env
  }

# âœ… ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ë°œí™” êµ¬ê°„ ìë¥´ê¸° â†’ ë…¸ì´ì¦ˆ ì œê±°)
def preprocess_audio(y, sr, label_data):
  # âœ… ë°œí™” êµ¬ê°„ ìë¥´ê¸° (SpeechStart ~ SpeechEnd)
  start_sample = int(label_data["speech_start"] * sr)
  end_sample = int(label_data["speech_end"] * sr)
  y_cropped = y[start_sample:end_sample]

  # âœ… ë…¸ì´ì¦ˆ ì œê±° ì ìš©
  y_denoised = nr.reduce_noise(y=y_cropped, sr=sr, stationary=True)

  return y_denoised  # âœ… trim()ì„ ì ìš©í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜

# âœ… MFCC ì¶”ì¶œ ë° ì €ì¥ í•¨ìˆ˜ (ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ ê¸°ëŠ¥ ì¶”ê°€)
def process_and_save_mfcc(file_path):
  path_parts = file_path.split('/')
  if len(path_parts) < 6:
    raise Exception(f"âš ï¸ íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜: {file_path}")

  room_id, call_label, date, user_id, file_name = path_parts[-5], path_parts[-4], path_parts[-3], path_parts[-2], path_parts[-1]

  if call_label != "call":
    raise Exception(f"âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥¸ í´ë” êµ¬ì¡°: {file_path}")

  file_name_without_ext = os.path.splitext(file_name)[0]

  # âœ… ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸ í›„ ìŠ¤í‚µ
  if file_path in processed_files:
    print(f"ğŸ”„ Skipping (Already Processed): {file_path}")
    return

  # âœ… ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ
  label_data = load_label_data(file_path)

  # âœ… ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬ ì ìš©
  y, sr = librosa.load(file_path, sr=None)
  y_processed = preprocess_audio(y, sr, label_data)

  # âœ… MFCC ë³€í™˜
  mfcc = librosa.feature.mfcc(y=y_processed, sr=sr, n_mfcc=14)

  # âœ… ì €ì¥í•  ê²½ë¡œ ì„¤ì •
  h5_save_dir = os.path.join(h5_output_base_dir, room_id, call_label, date, user_id)
  os.makedirs(h5_save_dir, exist_ok=True)
  h5_save_path = os.path.join(h5_save_dir, f"{file_name_without_ext}.h5")

  wav_save_dir = os.path.join(wav_output_base_dir, room_id, call_label, date, user_id)
  os.makedirs(wav_save_dir, exist_ok=True)
  wav_save_path = os.path.join(wav_save_dir, f"{file_name_without_ext}_processed.wav")

  # âœ… ì „ì²˜ë¦¬ëœ .wav íŒŒì¼ ì¶”ê°€ ì €ì¥
  if SAVE_WAV:
    sf.write(wav_save_path, y_processed, sr)
    print(f"âœ… ì €ì¥ ì™„ë£Œ (WAV): {wav_save_path}")

  # âœ… HDF5 íŒŒì¼ ì €ì¥
  with h5py.File(h5_save_path, "a") as h5f:
    h5f.create_dataset(file_name_without_ext, data=mfcc)
    h5f[file_name_without_ext].attrs["sr"] = sr
    h5f[file_name_without_ext].attrs["file_path"] = file_path
    h5f[file_name_without_ext].attrs.update(label_data)

  # âœ… ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ì— ì¶”ê°€ í›„ ì €ì¥
  processed_files[file_path] = h5_save_path
  save_progress(processed_files)

  print(f"âœ… ì €ì¥ ì™„ë£Œ (HDF5): {h5_save_path}")

# âœ… ëª¨ë“  WAV íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ ì¶”ê°€)
def process_all_wav_files(root_dir):
  file_count = 0
  for subdir, _, files in os.walk(root_dir):
    for file in files:
      if file.endswith(".wav"):
        file_path = os.path.join(subdir, file)
        process_and_save_mfcc(file_path)
        file_count += 1
        if file_count % 100 == 0:
          print(f"ğŸ“¢ Processed {file_count} files...")

  print(f"ğŸ‰ ì „ì²´ ë³€í™˜ ì™„ë£Œ! ì´ {file_count}ê°œ íŒŒì¼ì„ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")

# ì‹¤í–‰
process_all_wav_files(input_base_dir)


```
### 4.1.2 Feature ì¶”ì¶œ
- MFCCëŠ” (n x 14) í˜•íƒœë¡œ ì¶”ì¶œë˜ë©°, nì€ í”„ë ˆì„ ìˆ˜ì…ë‹ˆë‹¤.
- í”„ë ˆì„ ë‹¨ìœ„ì—ì„œ í‰ê· /í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•´ (2 x 14)ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
- ì´ë¥¼ ë‹¤ì‹œ ì‚¬ìš©ì ë‹¨ìœ„ë¡œ í‰ê· í•˜ì—¬ ìµœì¢… featureë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

- Feature ì¶”ì¶œ ì½”ë“œ
```python
import os
import h5py
import numpy as np
import pandas as pd
import json

# âœ… CPU ì„¤ì •
print("âœ… ì—°ì‚° ì¥ì¹˜: CPU")

# âœ… ê²½ë¡œ ì„¤ì •
root_dir = "/content/drive/MyDrive/SLink_PJT/Data/MFCC_noiserm_Features"
label_base_dir = "/content/drive/MyDrive/SLink_PJT/Data/voiceData/label"
ENERGY_THRESHOLD = 1e-3

user_features_dict = {}
user_gender_dict = {}

# âœ… ì„±ë³„ ìˆ«ì ë³€í™˜ í•¨ìˆ˜
def gender_to_numeric(gender_str):
    return 1 if gender_str.lower().startswith('m') else 0  # Male:1, Female:0

# âœ… MFCC CPU ì²˜ë¦¬ í•¨ìˆ˜
def process_h5_file_cpu(h5_path):
    with h5py.File(h5_path, 'r') as h5f:
        key = list(h5f.keys())[0]
        mfcc = np.array(h5f[key])

        frame_energy = np.sum(np.abs(mfcc), axis=0)
        valid_frames = mfcc[:, frame_energy > ENERGY_THRESHOLD]

        if valid_frames.shape[1] == 0:
            return None

        mean_mfcc = np.mean(valid_frames, axis=1)
        std_mfcc = np.std(valid_frames, axis=1)

        mfcc_feature = np.concatenate((mean_mfcc, std_mfcc))

    return mfcc_feature

# âœ… ì„±ë³„ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
def extract_gender(h5_path):
    label_path = h5_path.replace("MFCC_noiserm_Features", "voiceData/label").replace(".h5", ".json")
    if not os.path.exists(label_path):
        print(f"âš ï¸ ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {label_path}")
        return None
    with open(label_path, 'r') as f:
        label_data = json.load(f)
        gender = label_data["Speaker"]["Gender"]
    return gender_to_numeric(gender)

# âœ… ì „ì²´ ë°ì´í„° ì²˜ë¦¬ (CPU)
file_count = 0
processed_count = 0

for subdir, _, files in os.walk(root_dir):
    for file in files:
        if file.endswith('.h5'):
            file_count += 1
            h5_path = os.path.join(subdir, file)

            path_parts = h5_path.split('/')
            if len(path_parts) < 10:
                print(f"âš ï¸ ì˜ëª»ëœ ê²½ë¡œ êµ¬ì¡°: {h5_path}")
                continue

            user_id = path_parts[-2]

            mfcc_feature = process_h5_file_cpu(h5_path)
            if mfcc_feature is None:
                print(f"âš ï¸ ë¬´ìŒë§Œ ì¡´ì¬í•˜ì—¬ ì œì™¸: {h5_path}")
                continue

            gender_numeric = extract_gender(h5_path)
            if gender_numeric is None:
                continue

            user_features_dict.setdefault(user_id, []).append(mfcc_feature)
            user_gender_dict[user_id] = gender_numeric

            processed_count += 1
            if processed_count % 100 == 0:
                print(f"ğŸ”„ í˜„ì¬ê¹Œì§€ ì²˜ë¦¬í•œ íŒŒì¼ ìˆ˜: {processed_count}/{file_count}")


# âœ… Userë³„ í‰ê·  ë²¡í„° ê³„ì‚° (CPU)
user_ids, features_list, gender_list = [], [], []

for idx, (user_id, feature_list) in enumerate(user_features_dict.items(), 1):
    user_feature_mean = np.mean(feature_list, axis=0)

    user_ids.append(user_id)
    features_list.append(user_feature_mean)
    gender_list.append(user_gender_dict[user_id])

    if idx % 100 == 0:
        print(f"ğŸ”„ User íŠ¹ì§• ë²¡í„° ê³„ì‚° ì§„í–‰: {idx}/{len(user_features_dict)}ëª… ì™„ë£Œ")


# âœ… DataFrame ìƒì„± (ì„±ë³„ í¬í•¨)
mfcc_columns = [f'mfcc_mean_{i+1}' for i in range(14)] + [f'mfcc_std_{i+1}' for i in range(14)]
df_features = pd.DataFrame(features_list, columns=mfcc_columns, index=user_ids)
df_features.index.name = 'user_id'

# ì„±ë³„ ì»¬ëŸ¼ ì¶”ê°€
df_features['gender'] = gender_list

# âœ… DataFrameì„ í”¼í´(.pkl)ë¡œ ì €ì¥ (CPU)
output_df_path = "/content/drive/MyDrive/SLink_PJT/Data/user_mfcc_features_gender_cpu.pkl"
df_features.to_pickle(output_df_path)


```

## 4.2 íŠ¹ì§• Clustering
### 4.2.0 Clustering ê¸°ë²•
|       íŠ¹ì§•        |                                                                        	GMM Clustering                                                                         | 	DBSCAN | 	K-means Clustering |
|:---------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------:|:-------------------:|
|   Clusterì˜ ëª¨ì–‘   | 	ë°ì´í„°ì˜ Cluster ëª¨ì–‘ì´ Gaussian ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²ƒì„ ê°€ì •|	ë°ì´í„°ì˜ Cluster ëª¨ì–‘ì´ arbitraryí•˜ê²Œ ë¬¶ì´ëŠ” ê²½ìš° ì˜ Clustering ë¨. (ë¹„ì„ í˜• êµ¬ì¡°)	|ë°ì´í„°ì˜ Cluster ëª¨ì–‘ì´ Sphericalí•œ ê²½ìš°ì— ì˜ Clustering ë¨. (ë¹„ì„ í˜• êµ¬ì¡°) | 
|   Clusterì˜ ê°¯ìˆ˜   |                                       	.êµ°ì§‘í™”ë  ê°¯ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •í•´ì¤˜ì•¼ í•¨	|êµ°ì§‘í™” ê°¯ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •í•´ì£¼ì§€ ì•Šì•„ë„ ë¨(ë°€ë„ ê¸°ë°˜)|	êµ°ì§‘í™”ë  ê°¯ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •í•´ì¤˜ì•¼í•¨ (centroid ê¸°ë°˜)                                        |
|     Outlier     |                   	ë°ì´í„°ë¥¼ Gaussian ë¶„ë¡œ ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, ì˜ëª»ëœ ëª¨ë¸ë§ì´ ë  ìˆ˜ ìˆìŒ (Outlierì— ì·¨ì•½í•¨)	|Clusteringì— í¬í•¨ë˜ì§€ ì•ŠëŠ” Outlierë¥¼ íŠ¹ì •í•  ìˆ˜ ìˆìŒ|	ëª¨ë“  ë°ì´í„°ê°€ í•˜ë‚˜ì˜ Clusterì— í¬í•¨ë¨                    |  
| Initial Setting |                                    	ì´ˆê¸° êµ°ì§‘ ì¤‘ì‹¬, ì´ˆê¸° ê³µë¶„ì‚° í–‰ë ¬ì— ë”°ë¼ ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§|	ì´ˆê¸° Cluster ìƒíƒœê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ	|ì´ˆê¸° Centroid ì„¤ì •ì— ë”°ë¼ ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§                                    |  
| Computing Cost  |                                                         	ë†’ìŒ (EM ì•Œê³ ë¦¬ì¦˜)|	ë‚®ìŒ (K-means Clusteringë³´ë‹¤ëŠ” ë†’ìŒ)|	ë‚®ìŒ                                                         |     
|  Cluster ì†í•  í™•ë¥   |                                         	Gaussian ë‹¤ë³€ìˆ˜ ì •ê·œ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬, ê° Cluster í¬í•¨ë  í™•ë¥ ì„ ê³„ì‚° ê°€ëŠ¥|	ë°€ë„ ê¸°ë°˜ìœ¼ë¡œ ê°„ì ‘ ì¶”ì •|	ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ê°„ì ‘ ì¶”ì •                                         |  

### 4.2.1 ì´ìƒì¹˜ ì œê±° ì „ GMM Clustering(ì´ìƒì¹˜ ë¯¸ì œê±°ë¡œ ì¸í•œ ë¶„ë¥˜ê°€ ì˜ ì•ˆë¨)
- êµ°ì§‘í™”ë  ê°¯ìˆ˜ë¥¼ 10ê°œë¡œ í•´ì„œ ë¶„ë¥˜.
- ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ì§€ ì•Šê³ , GMMìœ¼ë¡œ Clusteringí•˜ì§€ ì•Šê³  UMAPì„ í™œìš©í•´ 2Dë¡œ proj.

![img.png](assets/Non-OutlierDetection.png)
```python
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import umap
import scipy.stats as stats

# âœ… ë°ì´í„° ë¡œë“œ
df = pd.read_pickle("/content/drive/MyDrive/SLink_PJT/Data/user_mfcc_features_gender_from_h5.pkl")
feature_cols = [f"mfcc_mean_{i+1}" for i in range(14)] + [f"mfcc_std_{i+1}" for i in range(14)]
X = df[feature_cols].values
X_scaled = StandardScaler().fit_transform(X)

# âœ… GMM í´ëŸ¬ìŠ¤í„°ë§
n_clusters = 10
gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42)
gmm_labels = gmm.fit_predict(X_scaled)

df['gmm_cluster'] = gmm_labels

# âœ… UMAP 2D ì„ë² ë”©
X_umap = umap.UMAP(n_components=2, random_state=42).fit_transform(X_scaled)
df['umap_1'] = X_umap[:, 0]
df['umap_2'] = X_umap[:, 1]

# âœ… ì €ì¥
output_path = f"/content/drive/MyDrive/SLink_PJT/Data/noneprep_foreuser_mfcc_gmm_{n_clusters}_result.pkl"
df.to_pickle(output_path)
print(f"âœ… GMM ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

# âœ… ì‹œê°í™”
plt.figure(figsize=(8, 6))
scatter = plt.scatter(df['umap_1'], df['umap_2'], c=df['gmm_cluster'], cmap='Spectral', s=10, alpha=0.7)
plt.colorbar(scatter, label='Cluster')
plt.title("UMAP + GMM Clustering")
plt.xlabel("UMAP 1")
plt.ylabel("UMAP 2")
plt.grid(True)
plt.tight_layout()
plt.show()

```
### 4.2.2 GMM ê¸°ë°˜ì˜ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬(Mahalanobis distance) ë¥¼ ì´ìš©í•´ ì´ìƒì¹˜(outlier)ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ íƒì§€í•˜ê³  ì œê±°
- ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ ê³„ì‚°
  - ë§¥ë½ì„ ì •ê·œí™” ì‹œí‚¤ê³ , ì´ë¥¼ í™œìš©í•´ì„œ ìœ í´ë¦¬ë“œê±°ë¦¬(d_E(x, y) = sqrt((x - y)^T * (x - y)))ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•
  - `d_M(x, Î¼) = sqrt((x - Î¼)^T * S^(-1) * (x - Î¼))`
- í‰ê·  ë²¡í„°ì™€ ê³µë¶„ì‚°ë¥¼ êµ¬í•˜ê³ , ì´ë¥¼ í™œìš©í•´ì„œ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ ê³„ì‚°
- í´ëŸ¬ìŠ¤í„° ë³„ ìƒìœ„ 1%ëŠ” ì´ìƒì¹˜ë¡œ íŒë‹¨í•˜ê³  ì œê±°

![img.png](assets/MahalanobisDistanceGMM.png)
